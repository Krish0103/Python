{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b87a0b12",
   "metadata": {},
   "source": [
    "# Web Scraping Resume Skills with BeautifulSoup\n",
    "\n",
    "This notebook demonstrates how to extract skillset information from an HTML resume file using BeautifulSoup.\n",
    "\n",
    "## Features:\n",
    "- Reads the resume HTML file\n",
    "- Parses and extracts the Skills section\n",
    "- Organizes skills by category\n",
    "- Displays formatted output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31bf3bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da8103ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched webpage. Status Code: 200\n",
      "Content Length: 8083 bytes\n"
     ]
    }
   ],
   "source": [
    "resume_url = input(\"Enter link to resume webpage (S3 URL): \")\n",
    "# e.g., \"https://your-bucket.s3.amazonaws.com/resume.html\"\n",
    "\n",
    "try:\n",
    "    # Send GET request to fetch the webpage\n",
    "    response = requests.get(resume_url)\n",
    "    response.raise_for_status()  # Raise an error for bad status codes\n",
    "\n",
    "    print(f\"Successfully fetched webpage. Status Code: {response.status_code}\")\n",
    "    print(f\"Content Length: {len(response.content)} bytes\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error fetching webpage: {e}\")\n",
    "    response = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ec7f793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML Structure Preview:\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1.0\" name=\"viewport\"/>\n",
      "  <title>\n",
      "   Resume - Krish Aggarwal\n",
      "  </title>\n",
      "  <style>\n",
      "   * {\n",
      "      margin: 0;\n",
      "      padding: 0;\n",
      "      box-sizing: border-box;\n",
      "    }\n",
      "    body {\n",
      "      font-family: 'Times New Roman', Times, serif;\n",
      "      max-width: 850px;\n",
      "      margin: 0 auto;\n",
      "      padding: 40px 50px;\n",
      "      line-height: 1.5;\n",
      "      color: #000;\n",
      "      background: #fff;\n",
      "    }\n",
      "    h1 {\n",
      "      text-align: center;\n",
      "      font-size: 28px;\n",
      "      font-weight: bold;\n",
      "      margin-bottom: 8px;\n",
      "    }\n",
      "    .contact-info {\n",
      "      text-align: center;\n",
      "      font-size: 11px;\n",
      "      margin-bottom: 20px;\n",
      "      display: flex;\n",
      "      justify-content: center;\n",
      "      align-items: center;\n",
      "      gap: 8px;\n",
      "      flex-wrap: wrap;\n",
      "    }\n",
      "    .contact-info a {\n",
      "      color: #0066cc;\n",
      "      text-decoration: none;\n",
      "    }\n",
      "    .contact-info a:hover {\n",
      "      text-decoration: underline;\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "# Step 2: Parse the HTML content\n",
    "if response:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Pretty print the HTML structure (first 1000 characters)\n",
    "    print(\"HTML Structure Preview:\")\n",
    "    print(soup.prettify()[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6639e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Extract Skills Section\n",
    "# This will try multiple common patterns for skills sections\n",
    "\n",
    "def extract_skills(soup):\n",
    "    \"\"\"\n",
    "    Extract skills from resume webpage using multiple strategies\n",
    "    \"\"\"\n",
    "    skills = []\n",
    "\n",
    "    # Strategy 1: Look for sections with 'skill' in id or class\n",
    "    skills_section = (\n",
    "        soup.find(id=lambda x: x and 'skill' in x.lower()) or\n",
    "        soup.find(class_=lambda x: x and 'skill' in str(x).lower()) or\n",
    "        soup.find('section', class_=lambda x: x and 'skill' in str(x).lower()) or\n",
    "        soup.find('div', class_=lambda x: x and 'skill' in str(x).lower())\n",
    "    )\n",
    "\n",
    "    if skills_section:\n",
    "        print(\"Found skills section using Strategy 1 (id/class matching)\")\n",
    "\n",
    "        # Extract text from list items\n",
    "        list_items = skills_section.find_all(['li', 'span', 'p'])\n",
    "        for item in list_items:\n",
    "            skill_text = item.get_text(strip=True)\n",
    "            if skill_text and len(skill_text) > 0:\n",
    "                skills.append(skill_text)\n",
    "\n",
    "        # If no list items, get all text\n",
    "        if not skills:\n",
    "            skills_text = skills_section.get_text(separator='\\n', strip=True)\n",
    "            skills = [line.strip() for line in skills_text.split('\\n') if line.strip()]\n",
    "\n",
    "    # Strategy 2: Look for headings containing 'skill'\n",
    "    if not skills:\n",
    "        print(\"Trying Strategy 2 (heading-based search)\")\n",
    "        headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "        for heading in headings:\n",
    "            if 'skill' in heading.get_text().lower():\n",
    "                # Get the next sibling elements\n",
    "                next_element = heading.find_next_sibling()\n",
    "                while next_element:\n",
    "                    if next_element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "                        break\n",
    "                    if next_element.name in ['ul', 'ol']:\n",
    "                        list_items = next_element.find_all('li')\n",
    "                        skills.extend([item.get_text(strip=True) for item in list_items])\n",
    "                    elif next_element.name in ['p', 'div']:\n",
    "                        text = next_element.get_text(strip=True)\n",
    "                        if text:\n",
    "                            skills.append(text)\n",
    "                    next_element = next_element.find_next_sibling()\n",
    "                if skills:\n",
    "                    break\n",
    "\n",
    "    # Strategy 3: Search for all lists and filter\n",
    "    if not skills:\n",
    "        print(\"Trying Strategy 3 (comprehensive list search)\")\n",
    "        all_lists = soup.find_all(['ul', 'ol'])\n",
    "        for lst in all_lists:\n",
    "            # Check if parent or previous sibling mentions skills\n",
    "            context = \"\"\n",
    "            if lst.find_previous(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "                context = lst.find_previous(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']).get_text().lower()\n",
    "\n",
    "            if 'skill' in context:\n",
    "                list_items = lst.find_all('li')\n",
    "                skills.extend([item.get_text(strip=True) for item in list_items])\n",
    "                break\n",
    "\n",
    "    return skills\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da1dbb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found skills section using Strategy 1 (id/class matching)\n",
      "\n",
      "==================================================\n",
      "EXTRACTED SKILLS\n",
      "==================================================\n",
      "1. Programming Languages:Java, JavaScript, C, C++, Python\n",
      "2. Backend & APIs:Spring Boot, RESTful Web Services, Node.js, Express.js, JWT Authentication\n",
      "3. Frontend:React.js, HTML, CSS, JavaScript\n",
      "4. Databases:MySQL, MongoDB\n",
      "5. Cloud & DevOps:AWS (EC2, basic deployment concepts), Git, GitHub\n",
      "6. Testing & Quality:API Testing (Postman), Debugging, Refactoring, Performance Optimization (Basics)\n",
      "7. Development Practices:Agile/Scrum, Microservices Architecture, Secure Coding Fundamentals\n",
      "\n",
      "Total skills found: 7\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Execute extraction and display results\n",
    "if response:\n",
    "    extracted_skills = extract_skills(soup)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EXTRACTED SKILLS\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    if extracted_skills:\n",
    "        for idx, skill in enumerate(extracted_skills, 1):\n",
    "            print(f\"{idx}. {skill}\")\n",
    "\n",
    "        print(f\"\\nTotal skills found: {len(extracted_skills)}\")\n",
    "    else:\n",
    "        print(\"No skills found. The HTML structure might be different.\")\n",
    "        print(\"\\nTip: Inspect the HTML structure manually to identify the skills section.\")\n",
    "        print(\"You can view the full HTML by uncommenting the line below:\")\n",
    "        print(\"# print(soup.prettify())\")\n",
    "\n",
    "    print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Skills saved to: extracted_skills.csv\n",
      "\n",
      "Skills DataFrame:\n",
      "   Skill_Number                                              Skill\n",
      "0             1  Programming Languages:Java, JavaScript, C, C++...\n",
      "1             2  Backend & APIs:Spring Boot, RESTful Web Servic...\n",
      "2             3           Frontend:React.js, HTML, CSS, JavaScript\n",
      "3             4                           Databases:MySQL, MongoDB\n",
      "4             5  Cloud & DevOps:AWS (EC2, basic deployment conc...\n",
      "5             6  Testing & Quality:API Testing (Postman), Debug...\n",
      "6             7  Development Practices:Agile/Scrum, Microservic...\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Save skills to a CSV file\n",
    "if response and extracted_skills:\n",
    "    # Create a DataFrame\n",
    "    skills_df = pd.DataFrame({\n",
    "        'Skill_Number': range(1, len(extracted_skills) + 1),\n",
    "        'Skill': extracted_skills\n",
    "    })\n",
    "\n",
    "    # Save to CSV\n",
    "    output_file = 'extracted_skills.csv'\n",
    "    skills_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nSkills saved to: {output_file}\")\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(\"\\nSkills DataFrame:\")\n",
    "    print(skills_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a5e8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Advanced extraction - Get all sections\n",
    "def extract_all_sections(soup):\n",
    "    \"\"\"\n",
    "    Extract all major sections from the resume\n",
    "    \"\"\"\n",
    "    sections = {}\n",
    "\n",
    "    # Find all headings\n",
    "    headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "\n",
    "    for heading in headings:\n",
    "        section_title = heading.get_text(strip=True)\n",
    "        section_content = []\n",
    "\n",
    "        # Get content until next heading\n",
    "        next_element = heading.find_next_sibling()\n",
    "        while next_element and next_element.name not in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "            text = next_element.get_text(strip=True)\n",
    "            if text:\n",
    "                section_content.append(text)\n",
    "            next_element = next_element.find_next_sibling()\n",
    "\n",
    "        if section_content:\n",
    "            sections[section_title] = section_content\n",
    "\n",
    "    return sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f75f4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Extract all sections for complete resume data\n",
    "if response:\n",
    "    all_sections = extract_all_sections(soup)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ALL RESUME SECTIONS\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    for section_title, content in all_sections.items():\n",
    "        print(f\"\\n### {section_title} ###\")\n",
    "        for item in content:\n",
    "            print(f\"  - {item}\")\n",
    "\n",
    "    print(\"=\"*50)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
